---
title: 'Introducing CAPE: Capability Engineering for AI'
publishedAt: '2025-12-05'
summary: 'A systematic practice for making models capable of what users and organisations actually need, reducing violation rates by 81% relative to DPO and cutting costs by 20–200×.'
---

[Read the full paper →](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub →](https://github.com/superficiallabs/cape)

A model that wins mathematical olympiads cannot reliably follow a hospital formulary. A model that writes publishable code cannot guarantee it will cite only the documents you provide. A model that passes the bar exam cannot enforce your firm's jurisdiction constraints.

This is the deployment gap: the chasm between what models can do in principle and what they can be trusted to do in practice. We have spent the last three years scaling intelligence, but we haven't scaled capability.

**Intelligence** is the ability to solve complex, open-ended problems. **Capability** is the reliable application of intelligence to specific requirements. A model can be extraordinarily intelligent and yet lack the capabilities that deployment demands. Intelligence is necessary but insufficient. Capability must be engineered.

Today we release CAPE (Capability Achievement via Policy Execution) and introduce **Capability Engineering**: a systematic practice for making models capable of what users and organisations actually need.

## Why Preference-Based Training Has Stalled

Post-training methods like RLHF optimise for intelligence. They ask annotators: "Which response is better?" But annotators disagree 30–50% of the time on subtle judgments. This isn't fixable with better guidelines or more training, it reflects genuine variance in human judgment when requirements are underspecified.

The consequences compound:

- **More compute amplifies noise.** Training harder against disagreeing annotators doesn't converge; it overfits to inconsistency.

- **Costs are prohibitive.** Preference annotation runs $50K–200K per capability area with timelines stretching to months.

- **The ceiling is fixed.** No amount of investment overcomes the preference plateau. Returns diminish while costs scale.

We don't need more intelligent models. We need more capable ones. And capability requires a different foundation.

## Contextual Objectivity

A common objection to specification-based approaches is: "You can't write specifications for the things that matter. Real requirements are too subjective."

This is wrong. It confuses *abstract* with *subjective*.

"Good medical advice" is abstract. It means different things to different people. But "recommend only drugs from this formulary, flag contraindications from the patient record, verify dosages against weight-based limits" is objective. There is a fact of the matter. It can be verified.

We call this **contextual objectivity**: the recognition that properties appearing subjective become objective once context is fixed. What seems like a matter of taste in the abstract becomes a testable requirement when you specify: good according to whom, for what purpose, under what constraints.

Our inter-annotator studies confirm this empirically:

<table>
  <thead>
    <tr>
      <th>Condition</th>
      <th>Agreement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Abstract ("Is this good medical advice?")</td>
      <td>63%</td>
    </tr>
    <tr>
      <td>Contextualised ("Formulary only, flag contraindications")</td>
      <td>85%</td>
    </tr>
    <tr>
      <td>Explicit Policy (executable specification)</td>
      <td>99%</td>
    </tr>
  </tbody>
</table>

The disagreement wasn't about values. It was about underspecified requirements. Fix the context, and agreement follows.

This is the unlock for making symbolic AI work: **most capability requirements that matter in production are contextually objective.** They're not matters of taste, they're matters of specification. And what can be specified can be verified. What can be verified can be trained.

## How CAPE Works

CAPE implements a closed loop: **Specify → Verify → Correct → Train.**

**Specify.** Requirements become executable. For structural properties (format compliance, citation presence, formulary membership), we use the CAPE Policy Language (CPL)—a deterministic specification language. For semantic properties requiring judgment (reasoning validity, plan feasibility), we use learned verifiers trained on explicit rubrics.

**Verify.** Model outputs are parsed into a structured representation (the PredicateGraph) and evaluated against specifications. Symbolic policies execute deterministically: same output, same verdict, every time. Learned verifiers output scores with identified issues. Both produce actionable diagnostics—not just "this failed" but "argument 7.1 does not match computed result 7.095."

**Correct.** Violations trigger targeted fixes. Deterministic patches where possible, constrained rewrites otherwise. Every correction is re-verified before acceptance.

**Train.** Corrected outputs become supervised training signal. No preference labels required.

This is capability engineering: the same discipline that governs traditional software—explicit requirements, verifiable correctness, traceable failures, validated fixes—applied to model behaviour.

## The Loop Compounds

Here's what changes everything: **CAPE's gains compound.**

Better models improve extraction fidelity. Better extraction enables stricter specifications. Stricter specifications generate cleaner corrections. Cleaner corrections produce more capable models. The cycle continues.

Preference-based methods face the opposite dynamic. Annotator disagreement is fixed at 30–50% regardless of model capability. More compute yields diminishing returns. You hit the ceiling and stop.

With CAPE, the ceiling is technical—extraction accuracy, verifier fidelity—and technical ceilings fall with scale. We measure this directly: violation rate correlates with extraction error at r=0.94. As extraction improves (which it does with each model generation), violation rates fall proportionally.

**Compute investment yields guaranteed returns again.** This is the scaling law that preference-based training lost.

## What This Unlocks

### Post-training for everyone

Preference annotation costs $50K–200K per capability area. CAPE costs ~$500. Timelines drop from months to weeks.

This isn't marginal improvement—it's a category shift. Capability-specific post-training is no longer reserved for frontier labs with annotation budgets. Any organisation with clear requirements can now train models to satisfy them.

<table>
  <thead>
    <tr>
      <th></th>
      <th>RLHF/DPO</th>
      <th>CAPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Specification</td>
      <td>1–2 weeks</td>
      <td>1–3 days</td>
    </tr>
    <tr>
      <td>Data</td>
      <td>$20K–50K</td>
      <td>$200</td>
    </tr>
    <tr>
      <td>Training</td>
      <td>1–2 weeks</td>
      <td>1–3 days</td>
    </tr>
    <tr>
      <td><strong>Total</strong></td>
      <td><strong>$50K–200K</strong></td>
      <td><strong>~$500</strong></td>
    </tr>
    <tr>
      <td><strong>Timeline</strong></td>
      <td><strong>3–6 months</strong></td>
      <td><strong>1–2 weeks</strong></td>
    </tr>
  </tbody>
</table>

### Engineering, not alchemy

Traditional software has specifications, test suites, and traceable fixes. When code fails, you know what broke. When you fix it, you prove it's fixed.

Language models have had benchmarks, preferences, and hope.

CAPE brings engineering discipline to model behaviour. Policies are explicit artifacts—auditable, versionable, shareable. When a model fails in production, you can trace the failure to a specific policy. When you improve the model, you can prove the improvement against the same specification.

Capability becomes a reliability problem. And reliability problems are solvable.

### The right models for deployment

Consider what deployment actually requires:

- A legal research assistant must follow your firm's citation style, respect jurisdiction constraints, and maintain confidentiality rules

- A customer service agent must follow your escalation protocol, apply your refund policy, and maintain your brand voice

- A medical assistant must recommend only from the approved formulary, flag contraindications, and verify dosages

No general-purpose model satisfies these out of the box. Intelligence doesn't help—these are capability requirements. With CAPE, organisations can specify exactly what they need and train models that reliably deliver it.

## Results

Across 102,000 examples in six domains, CAPE reduces violation rates by 81% relative to DPO:

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Arithmetic</th>
      <th>Code Safety</th>
      <th>Citation</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>31.4%</td>
      <td>38.2%</td>
      <td>35.7%</td>
      <td>35.1%</td>
    </tr>
    <tr>
      <td>DPO</td>
      <td>10.2%</td>
      <td>15.8%</td>
      <td>13.4%</td>
      <td>13.1%</td>
    </tr>
    <tr>
      <td>CAPE (open-weight)</td>
      <td>4.1%</td>
      <td>6.9%</td>
      <td>5.8%</td>
      <td>5.6%</td>
    </tr>
    <tr>
      <td>CAPE (frontier)</td>
      <td>1.8%</td>
      <td>3.2%</td>
      <td>2.6%</td>
      <td>2.5%</td>
    </tr>
  </tbody>
</table>

With frontier extractors, residual violations fall to 2.5%. Critically, CAPE exhibits high stability: σ < 0.3% across random seeds, compared to 1.6–2.1% for reward-based methods. For symbolic policies, verification is deterministic—same output, same verdict, every time.

Hybrid training works too. CAPE first, then DPO, achieves 2.9% violation rate with 63.7% preference win rate. Policies establish the capability floor; preferences optimise quality within constraints.

## What We're Releasing

Everything needed to start building with CAPE is available under Apache 2.0:

- **CAPE Protocol**: The complete Specify → Verify → Correct → Train methodology

- **PredicateGraph Schema**: Structured representation for model outputs

- **CPL Specification**: The CAPE Policy Language for writing executable policies

- **Policy Packs**: Ready-to-use policies for arithmetic, code safety, citation, medical, legal, PII protection, customer service, and financial domains

- **CapabilityBench**: Public capability registry for evaluating models against community-contributed policies

### CapabilityBench: A Call to the Community

Current benchmarks measure intelligence with opaque aggregate scores. A model scoring 78% tells you nothing about whether it can satisfy your specific requirements.

CapabilityBench is a public capability registry at capabilitybench.com. We release the policy pack specification and initial packs under Apache 2.0. Organizations can implement evaluation against these packs using any CAPE-compatible runtime; results are submitted to capabilitybench.com for publication after review.

Results show exactly which requirements each model satisfies or violates—not "how smart" but "capable of what."

We're releasing eight initial policy packs, but the vision is larger: **a shared, growing repository of capability specifications across every domain that matters.** As organisations contribute policy packs encoding their deployment requirements, CapabilityBench becomes the definitive resource for understanding model capability—not in the abstract, but against the specific constraints that determine whether deployment is possible.

We invite researchers, practitioners, and organisations to contribute policy packs for your domains. The future of evaluation is not better benchmarks—it's explicit specifications.

## The Path Forward

Post-training is at an inflection point. The preference-based paradigm delivered remarkable progress, but its ceiling is now visible. Annotators disagree. Algorithms introduce bias. Returns diminish. The path to more capable models does not run through more preferences.

Verification offers a different path. One where requirements are explicit, failures are traceable, and improvement compounds with scale. One where any organisation with clear requirements can train models to satisfy them. One where capability becomes an engineering discipline.

We're releasing CAPE openly because we believe the answer to capable AI is not proprietary methods held by a handful of organisations, but rigorous frameworks developed in public where the global community can stress-test, extend, and improve them.

Executable specifications are auditable by design. That auditability should be a shared resource.

**We don't need more intelligent models. We need more capable ones.**

**Capability must be engineered. Now it can be.**

[Get started →](https://github.com/superficiallabs/cape)
