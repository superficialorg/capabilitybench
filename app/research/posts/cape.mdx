---
title: 'Introducing CAPE: Breaking Through the Post-Training Scaling Wall'
publishedAt: '2025-11-24'
summary: 'An open-source framework that replaces expensive preference annotation with LLM-assisted policy authoring, making post-training transparent, auditable, and accessible.'
---

At Superficial Labs, our goal is simple but non-negotiable: accelerate human progress by making AI capable and reliable enough for the world's most important work.

But today, the primary bottleneck standing in the way of that future is clear.

Post-training has stopped scaling. CAPE exists because that must change.

## The Problem: Post-Training Can No Longer Keep Up

Modern post-training depends on preference learning — asking humans (or models) to decide which of two outputs is "better". This worked when models were weaker. It fails when they are good.

As capability rises, outputs become harder to distinguish. Disagreement rates spike. And under standard models of preference learning like Bradley–Terry, the supervision signal collapses as disagreement approaches 0.5. At that point, no amount of compute produces proportionate improvement.

Labs across the field have run into the same plateau:

* math

* code

* tool use

* citation-grounded QA

* structured reasoning

The third stage of training — the one responsible for safety, correctness, style, and compliance — has become the hard ceiling on model quality.

If we want AI to reliably support the world's most consequential work, this ceiling must be broken.

## What CAPE Is

CAPE (Capability Achievement via Policy Enforcement) reframes post-training entirely.

Instead of learning to approximate human preferences, models learn to satisfy explicit, auditable, executable capability policies.
These policies can be written by humans or, critically, drafted by LLMs and validated by domain experts.

CAPE turns post-training into a transparent, deterministic process — one that gets more reliable as models get better.

## Why CAPE Matters

CAPE directly addresses the four structural problems limiting today's post-training systems:

1. **Restored scaling**

Supervision noise is bounded by extractor fidelity (ε), not human disagreement (δ).

2. **Massive cost reduction**

LLM-assisted policy authoring replaces large-scale preference annotation.

3. **Reusability of existing RLHF datasets**

Structural invariants in preference data can be distilled into permanent, reusable policies.

4. **Unified enforcement across three capability tiers**

* Tier 1: Objective correctness

* Tier 2: Safety and governance

* Tier 3: Structural style and voice

This aligns directly with Superficial's mission: ensure AI capability and reliability advance together, not at odds.

## How CAPE Works (At a High Level)

CAPE's mechanism is minimal and robust:

1. Extract a structured PredicateGraph from model outputs using an LLM.

2. Evaluate executable policies over that graph.

3. Correct violations deterministically or with constrained rewrites.

4. Train the model on corrected outputs.

This transforms post-training from a noisy preference-matching exercise into a mathematically crisp capability achievement loop.

## What We Showed

Across 30,000 examples in arithmetic with tool calls, safe Python generation, and citation-grounded QA, we observe the same pattern:

* DPO/RLHF reduce violations initially but plateau between 10–20%.

* CAPE continues improving beyond these plateaus.

* CAPE reaches extraction-limited floors of 2–8%, depending on extractor strength.

* Residual violations correlate with extractor error, not model capacity.

In short: CAPE keeps improving exactly where preference methods stop.

## What This Makes Possible

CAPE advances Superficial's mission by enabling models that are both more capable and more reliable in domains where failure matters:

* Auditable training signals for regulated industries

* Consistent correctness for math, code, and tool use

* Stable compliance for medical, financial, and legal advice

* Custom behaviours defined directly by domain experts

* Reusable policy packs that survive across model generations

This is post-training that is transparent, maintainable, and scalable — essential ingredients for AI systems entrusted with high-stakes decisions.

## What We're Releasing

Today we are releasing CAPE-Core, an open-source implementation including:

* PredicateGraph schema

* Reference extractors for math, code, and QA

* Deterministic policy engine

* LLM-assisted policy authoring workflows

* Correction engine (literal, template, constrained rewrite)

* Starter policy packs

* Evaluation harness

* Documentation suitable for domain experts

This is the beginning of a shared ecosystem of executable capability policies — a foundation for transparent, reliable post-training across the field.

## What Comes Next

CAPE is not a replacement for preference learning.

It is the structural layer that makes preference learning meaningful again.

Use CAPE to enforce correctness, safety, governance, and structural scaffolding.

Use preferences to optimize subjective quality inside those constraints.

This hybrid — CAPE first, preferences second — is the most promising path we see toward post-training that continues to scale as model capability increases.

And scaling capability and reliability together is precisely how we accelerate human progress.

## Get Involved

* Read the paper → /mnt/data/CAPE (Superficial Labs).pdf

* [Try CAPE-Core →](https://github.com/superficiallabs/cape-core)

* Join the community → Discord/Slack link

We welcome contributions in:

* domain-specific extractors

* new capability policies

* improved extraction methods

* best practices for LLM-assisted policy authoring

* case studies from production deployments

CAPE is a step toward AI systems that can be trusted in domains where trust truly matters.

That is the future we're working to accelerate — and we invite you to build it with us.
