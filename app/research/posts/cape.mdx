---
title: 'Introducing CAPE: A New Paradigm for Post-Training'
publishedAt: '2025-12-01'
summary: 'A post-training protocol that replaces preference collection with executable verification, reducing violation rates by 74% relative to DPO and cutting costs by 20–200×.'
---

[Read the full paper →](https://arxiv.org/abs/XXXX.XXXXX) &nbsp; [GitHub →](https://github.com/superficiallabs/cape)

Post-training has reached an impasse. Methods like RLHF and DPO depend on human preferences, but human annotators disagree on 30–50% of subtle judgments. This disagreement creates a noise floor that no amount of compute can overcome. Budgets run to six figures per capability area. Timelines stretch to months. And models still hallucinate facts, skip safety checks, and produce flawed reasoning in production.

We believe the path forward requires a different foundation. Today we release CAPE (Capability Achievement via Policy Execution), a post-training protocol that replaces preference collection with executable verification. 

Our core insight: most capabilities that matter are not matters of taste. Correctness, safety, and valid reasoning are inherently objective. Even properties that seem subjective - "good customer service," "clear explanation" - become objective once context is fixed. "Acknowledge the issue before offering solutions; offer escalation when unresolved" is not a preference. It is a testable requirement. We call this contextual objectivity: the recognition that organizational and domain context renders most apparently subjective properties into verifiable specifications.

We are open-sourcing the core framework under Apache 2.0: the PredicateGraph schema, the CAPE Policy Language specification, and example policy packs for arithmetic, code safety, and citation grounding. Everything is available at github.com/superficiallabs/cape, with complete technical details in the accompanying paper.

## The Problem: Preferences Hit a Ceiling

In software engineering, requirements are executable. When code fails a test, you know exactly what broke. When you fix it, you prove it's fixed. The feedback loop is tight and deterministic.

Language models have no equivalent. "Be helpful" is an aspiration, not a specification. When a model fails, there is no trace to a specific requirement. When annotators label outputs, they disagree with each other as often as they agree on difficult cases. Training on this signal means training on noise.

The consequences appear everywhere. Agents make wrong API calls and cannot recover. Mathematical proofs reach correct answers through invalid steps. Generated code passes tests while harboring security vulnerabilities. The common thread: outcome-based rewards provide no signal about process correctness. And when you scale compute against noisy signal, you amplify the noise.

## The Solution: Verification as Training Signal

CAPE operates on a simple principle: if you can verify a property, you can train for it directly. The protocol forms a closed loop—Specify, Verify, Correct, Train—where every failure generates structured signal and every correction strengthens the system.

Specify. Requirements become executable. For structural properties like format compliance, arithmetic accuracy, and citation presence we introduce the CAPE Policy Language (CPL), a deterministic specification language. For semantic properties that require reasoning like proof validity and plan feasibility we use learned verifiers trained on explicit rubrics rather than implicit preferences.

Verify. Model outputs are parsed into a structured representation (the PredicateGraph), then evaluated against specifications. Symbolic policies execute deterministically with zero variance. Learned verifiers output scores alongside identified issues. Both produce actionable diagnostics: not just "this failed," but "argument 7.1 does not match computed result 7.095."

Correct. Violations trigger deterministic patches where possible, constrained rewrites otherwise. Every correction is re-verified before acceptance.

Train. The corrected outputs become supervised training signal. No preference labels required.

This loop compounds. Better models improve extraction fidelity, which enables stricter specifications, which generate cleaner corrections, which produce stronger models. Unlike preference learning, where additional compute amplifies annotator disagreement, CAPE's ceiling is technical, and technical ceilings fall with scale.

## What Makes a Property Verifiable

A common objection with symbolic systems is that explicit verification only works for narrow, objective tasks. Everything important is too subjective for specification.

This conflates two distinctions. 

Structural versus semantic: can the property be checked by pattern-matching, or does it require understanding meaning? 

Objective versus subjective: is there a fact of the matter, or does it depend on individual taste?

But semantic does not imply subjective. "The proof step is logically valid" requires understanding (semantic) but has a definite answer (objective). "This explanation is elegant" depends on preference (subjective). The former is verifiable; the latter is not.

Most capabilities that matter in deployment such as correctness, safety, reasoning validity, plan feasibility are objective. Many properties that seem subjective become objective once context is fixed. "Good customer service" is vague in the abstract. "Acknowledge the issue before offering solutions; offer escalation when unresolved" is testable. We call this contextual objectivity: the recognition that organizational requirements render most "subjective" properties into verifiable specifications.

CAPE handles both structural properties (via symbolic CPL policies) and semantic properties (via learned verifiers with explicit rubrics). Only genuinely subjective properties like aesthetic quality and creative excellence require preference learning (and even in some contexts these too can be expressed as policies).

## Results

Across 90,000 examples spanning arithmetic with tool use, safe code generation, and citation-grounded question answering, CAPE reduces violation rates by 74% relative to DPO. With frontier extractors, residual violations fall to 3.8%. Critically, residual error tracks extraction fidelity (r = 0.94), confirming that the binding constraint has shifted from human disagreement to technical accuracy.

For semantic verification, recent work provides independent validation. 

DeepSeekMath-V2's IMO gold-medal theorem prover follows a semantic verification pattern: explicit rubrics rather than preferences, issue identification as a required output, meta-verification to catch hallucinated critiques, and verifier-as-reward-model for generator training. Their results show CAPE's framework extends to the hardest reasoning tasks.

## Economics

CAPE reduces capability-specific post-training costs by 20–200×. A pipeline processing 10,000 examples costs approximately $500 total: extraction, evaluation, correction, and fine-tuning. The equivalent preference-based approach runs $50,000–500,000 with timelines of months rather than days.

The scaling economics differ as well. Adding capability areas under preference methods requires new annotation campaigns. Under CAPE, new capabilities require new policies but reuse the same infrastructure. Marginal cost per additional capability area drops to $500–1,000.

This opens post-training to organizations that couldn't justify six-figure annotation budgets. Any team with clear requirements such as compliance rules, brand guidelines, workflow protocols can now enforce them directly and benefit from custom post-training.

## Beyond Training: Runtime Verification

CAPE is useful beyond training. The same infrastructure that generates training signal can guide inference. When a model fails to solve a problem on the first attempt, it can generate multiple candidates, verify each against the relevant policies, and either select the best or use the identified issues to prompt a refined attempt. DeepSeekMath-V2 demonstrated this at scale: Pass@1 on IMO Shortlist problems improved from 0.15 to 0.27 over eight refinement iterations. Deployed models become more robust without retraining.

## Working with Us

We're excited to open source everything needed to start experimenting with CAPE. The PredicateGraph schema defines a model output's structured representation. The CPL specification describes how to write policies. The example policy packs for arithmetic, code safety, and citation grounding offer working templates to adapt for your own requirements. For many use cases—particularly those with well-defined structural properties—this is enough to begin generating verification-based training signal today.

For organizations deploying CAPE at scale, we offer direct integration support. This includes optimized extractors, policy authoring via the Policy Architect, and deep expertise in identifying which properties in your domain are contextually objective and how to specify them. We work with frontier labs and enterprises to embed CAPE into production pipelines.

We are particularly interested in collaborating with regulators, safety researchers, and industry leaders to develop open-source policy packs for critical domains. The future of alignment lies not in vague guidelines but in executable specifications. We would welcome the opportunity to define those standards together.

## Get Started

To read the full paper and access the open-source release, visit github.com/superficiallabs/cape.

We believe post-training is at an inflection point. The preference-based paradigm has delivered remarkable progress, but its ceiling is now visible. Verification offers a path beyond that ceiling, one where capability requirements are explicit, failures are traceable, and improvement compounds with scale. This is not a marginal refinement. It is a different way of thinking about what it means to align a model.

But new paradigms require new norms. We are releasing CAPE openly because we believe the answer to AI alignment is not proprietary methods held by a handful of organizations, but rigorous frameworks developed in public where the global research community can stress-test, extend, and improve them. 

Executable specifications are auditable by design. That auditability should be a shared resource.

There is a window today to establish open verification as a foundation for post-training—to define the policies, schemas, and standards that will shape how capable models behave. That window will not stay open indefinitely. If this mission resonates, we hope you will join us.
