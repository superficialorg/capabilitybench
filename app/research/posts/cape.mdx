---
title: 'Introducing CAPE: Capability Engineering for AI'
publishedAt: '2025-12-05'
summary: 'A new protocol for post-training that closes the gap between what models can do and what they will do.'
---

[Read the full paper →](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub →](https://github.com/superficiallabs/cape)

Today we're releasing CAPE (Capability Achievement via Policy Execution), a new protocol for post-training that closes the gap between what models can do and what they will do.

## The Problem

You deploy a model at a law firm. It writes brilliant case analysis, constructs compelling arguments, explains precedents clearly. Then it cites a Ninth Circuit decision for a Second Circuit case. The legal reasoning was sound. The case law was relevant. But it violated the firm's jurisdiction hierarchy: Second Circuit > SDNY > other circuits. The model never learned that this firm's definition of "thorough research" includes jurisdiction-specific citation priorities.

Or you deploy it for financial advisory. It provides sophisticated portfolio analysis, explains risk-adjusted returns, suggests diversification strategies. Then it recommends a product that's not on your approved list. The advice was technically correct. But it violated your compliance requirements. The model didn't know that "good financial advice" at your firm means approved products only.

The model isn't stupid. It's intelligent. What it lacks is capability: knowing what intelligence means in your specific context.

This isn't a rare edge case. It's the deployment gap. And it's why most AI projects stall somewhere between "impressive demo" and "production system."

## Intelligence ≠ Capability

We've been conflating two distinct properties:

**Intelligence** is the raw ability to solve complex, open-ended problems. It's general-purpose. It's what benchmarks measure. It's what makes demos impressive.

**Capability** is intelligence applied to a context. It's knowing that "good advice" at Goldman means something different than at a regional credit union. It's understanding that "thorough research" at Cravath has different requirements than at a solo practice.

A model can be highly intelligent yet lack specific capabilities. It can write brilliant legal analysis while citing cases from the wrong jurisdiction. It can give financially sound advice while recommending non-approved products. It can produce elegant code while violating your security policies.

Intelligence is context-free. Capability is context-complete.

## The Impossible Problem That Isn't

Here's the trap the field walked into: trying to make models "good at legal research" universally.

But there's no such thing. Legal research at Cravath means something different than at a solo practice. Different jurisdictions, different citation standards, different escalation triggers. "Universal legal research" is a contradiction—like trying to write universally good advice that applies to everyone, everywhere.

General capability doesn't exist. Capability is the application of intelligence to a context. No context? No capability.

The magic happens when you stop asking "how do we make models better at everything?" and start asking "how do we make this model embody what 'good' means here?" The first question has no answer. The second has a protocol.

CAPE doesn't solve the universal problem. It makes the universal problem irrelevant.

## The Context Flip

Consider "good customer service." At the general level, it's genuinely subjective. Annotators will disagree forever.

But for your company, it decomposes into specific requirements:

- Acknowledge the customer's issue before offering solutions
- Offer escalation when sentiment indicates frustration
- Never promise specific timelines without checking availability
- Reference the customer's history when relevant
- Match formality to the customer's tone

These aren't just "reliability constraints." They're what good customer service means at your company. They encode your values, your brand, your operational reality. A model that satisfies them isn't just "not making mistakes"—it's being good at customer service in your context.

Our inter-annotator study confirms this: agreement jumps from κ = 0.42 ("Is this good customer service?") to κ = 0.98 ("Does this satisfy these specific requirements?"). Context doesn't just reduce errors—it makes evaluation possible.

## Why Current Methods Hit a Ceiling

RLHF and DPO try to solve the general problem. They ask annotators: "Which response is better?" But better for what? Better where? Better according to whom?

The results are predictable:

- Annotators disagree 30-50% of the time because they're bringing different contexts to the judgment
- Models learn averaged preferences that belong to no actual deployment context
- The ceiling is structural: more compute just amplifies the disagreement

You're not training toward a target. You're training toward an average of conflicting targets. No amount of data fixes this.

You end up with models that are generically pleasant but specifically useless—good at being inoffensive, bad at being right for your context.

## Introducing Capability Engineering

Capability engineering is the systematic practice of defining what intelligence means in a specific context, then training models to embody that definition.

<table>
  <thead>
    <tr>
      <th>Layer</th>
      <th>Function</th>
      <th>Mechanism</th>
      <th>Guarantee</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Context Engineering</td>
      <td>Inform</td>
      <td>RAG, retrieval</td>
      <td>Probabilistic</td>
    </tr>
    <tr>
      <td>Prompt Engineering</td>
      <td>Guide</td>
      <td>Instructions</td>
      <td>Probabilistic</td>
    </tr>
    <tr>
      <td>Capability Engineering</td>
      <td>Define</td>
      <td>Specifications</td>
      <td>Verifiable</td>
    </tr>
  </tbody>
</table>

The key insight: capability requirements become objective once context is fixed.

"Good financial advice" is subjective. "Recommend only approved products, disclose all fees, verify suitability against stated risk tolerance, flag high-risk strategies for supervisor review" is objective—and it's also what good financial advice means at this firm.

Specifications aren't constraints on intelligence. They're definitions of what intelligence means in practice. Without them, "good" is meaningless.

## CAPE: The Protocol

CAPE operationalizes capability engineering through a closed loop:

**Specify → Verify → Correct → Train**

**1. Specify**: Define what "good" means in your context. Convert requirements into executable policies (CPL for structural properties) or rubrics (for semantic properties like reasoning quality).

**2. Verify**: Evaluate outputs against your definition. For structural properties, verification is deterministic. For semantic properties, learned verifiers trained on explicit rubrics provide reliable assessment.

**3. Correct**: When outputs don't meet your standard, generate ones that do. Deterministic patching for clear fixes, constrained rewrite for judgment calls.

**4. Train**: Fine-tune on examples that embody your definition. The model learns what "good" means here.

The model doesn't just avoid mistakes—it internalizes your context. After training, it thinks like your organization by default.

## CAPE Compounds

Here's what makes CAPE different from one-shot improvements: the loop feeds itself.

- Better verification → cleaner training signal
- Cleaner signal → better models
- Better models → improved verification
- Improved verification → more nuanced specifications

This isn't aspirational—it's empirical. Our learning curves show DPO plateauing at ~12% violation rate while CAPE continues improving throughout training. The gap widens over time.

And the ceiling keeps falling. Extraction fidelity improved from 72% to 96% in eighteen months. Each percentage point translates to better capability transfer. CAPE gets better automatically as the infrastructure improves.

## Post-Training Just Became Accessible

The dirty secret of RLHF: annotation costs dominate everything.

At $5-15 per preference comparison, training a model on 10,000 examples requires $50,000-150,000 in annotation. Want to add a new capability domain? New annotation campaign. New budget. New timeline.

CAPE inverts this:

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>RLHF/DPO</th>
      <th>CAPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Specification</td>
      <td>—</td>
      <td>$2,000-4,000</td>
    </tr>
    <tr>
      <td>Annotation</td>
      <td>$50,000-150,000</td>
      <td>—</td>
    </tr>
    <tr>
      <td>Verification infrastructure</td>
      <td>—</td>
      <td>$200-400</td>
    </tr>
    <tr>
      <td>Compute (training)</td>
      <td>$8,000-12,000</td>
      <td>$8,000-12,000</td>
    </tr>
    <tr>
      <td><strong>Total cost</strong></td>
      <td><strong>$80,000-200,000</strong></td>
      <td><strong>$10,000-16,000</strong></td>
    </tr>
    <tr>
      <td><strong>Timeline</strong></td>
      <td><strong>2-4 months</strong></td>
      <td><strong>1-2 weeks</strong></td>
    </tr>
  </tbody>
</table>

The difference: annotation costs scale O(n) with examples. Policies scale O(1)—define once, apply everywhere.

Our arithmetic policy pack took 3 days to write. It has generated training signal for 50,000+ examples. The legal pack encodes jurisdiction logic that would take months to capture through annotation.

## Specifications Are Your Moat

Here's something the economics obscure: your specifications encode what makes you *you*.

Your jurisdiction priorities. Your risk tolerance thresholds. Your escalation logic. Your documentation standards. Your brand voice. These aren't generic capabilities—they're the operational knowledge that defines your organization.

With RLHF, you're buying generic preference data that makes models generically helpful. With CAPE, you're encoding your definition of good. The specification is your competitive advantage.

And specifications compose. Legal reasoning standards + jurisdiction rules + client confidentiality requirements + your escalation protocol = a capability stack that embodies your firm's judgment, built from reusable components.

## The Verification-Fidelity Scaling Law

Here's what changes everything: verification fidelity improves with scale, preference agreement doesn't.

Preference disagreement is fixed at 30-50%—it reflects the absence of shared context, not measurement error. No amount of compute changes this.

But extraction accuracy and verifier capability both improve as models get better:

- Each percentage point reduction in extraction error → ~0.8pp reduction in violation rate (r = 0.94)
- Learned verifier accuracy correlates strongly with downstream model quality (r = 0.91)

The ceiling for preference methods is human disagreement. The ceiling for CAPE is technical. Technical ceilings fall.

## Results

Enough theory. Here's what happens when you train with specifications instead of preferences.

Across 109,500 examples in six domains:

### Structural Verification (Violation Rates)

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Arithmetic</th>
      <th>Code Safety</th>
      <th>Citation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>31.4%</td>
      <td>38.2%</td>
      <td>35.7%</td>
    </tr>
    <tr>
      <td>DPO</td>
      <td>10.2%</td>
      <td>15.8%</td>
      <td>13.4%</td>
    </tr>
    <tr>
      <td>CAPE</td>
      <td>1.8%</td>
      <td>3.2%</td>
      <td>2.6%</td>
    </tr>
  </tbody>
</table>

CAPE reduces violation rates by 81-99% relative to DPO.

### Semantic Verification (Rubric Scores)

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Argument Soundness</th>
      <th>Proof Validity</th>
      <th>Code Correctness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DPO</td>
      <td>0.62</td>
      <td>0.58</td>
      <td>0.64</td>
    </tr>
    <tr>
      <td>CAPE</td>
      <td>0.83</td>
      <td>0.81</td>
      <td>0.84</td>
    </tr>
  </tbody>
</table>

CAPE works across the spectrum—from deterministic structural properties to nuanced semantic judgments.

**Stability**: σ < 0.3% across random seeds (vs. 1.6-2.1% for reward-based methods)

**Hybrid Training**: CAPE → DPO achieves 2.9% violation rate and 63.7% preference win rate. Define correctness first, then optimize for fluency within those bounds.

## CapabilityBench: Evaluation That Matters

Current benchmarks tell you how smart a model is. CapabilityBench tells you what it can do for you.

Instead of "GPT-5 scores 87% on MMLU," you get:

- **Medical Assistant**: 94% formulary adherence, 98% contraindication flagging
- **Legal Research**: 89% jurisdiction compliance, 96% citation accuracy
- **Customer Service**: 91% escalation protocol adherence, 97% PII protection

Community-contributed policy packs mean evaluation grows with real deployment needs. Your context, your definition of good, your results.

Because "78% on a benchmark" tells you nothing. "94% compliance with your policies" tells you everything.

## When to Use CAPE

CAPE excels when you can define what "good" means:

- Domain-specific standards (your legal research process, your medical protocols)
- Organizational requirements (your escalation logic, your documentation standards)
- Structural correctness (arithmetic, citations, format compliance)
- Reasoning quality (with explicit rubrics for what counts as sound)

Preferences remain essential when "good" is genuinely subjective:

- Aesthetic quality
- Creative excellence
- Humor

Our analysis: 89% of deployment requirements become objective once context is fixed.

## Who Should Use CAPE

**You're an AI engineer at an enterprise:**
You need models that follow your company's rules, not generic helpfulness. CAPE lets you encode your context without six-figure annotation budgets.

**You're a researcher:**
You want to push verification-fidelity scaling. CAPE gives you a protocol to test learned verifiers, rubric design, and extraction improvements.

**You're building a product:**
Your users have domain-specific needs. CAPE lets you customize models for niche contexts without retraining from scratch.

## What We're Releasing (and What We're Inviting)

Everything under Apache 2.0:

- **CAPE Protocol**: The complete Specify → Verify → Correct → Train loop
- **PredicateGraph Schema**: Structured intermediate representation for model outputs
- **CPL Specification Language**: Executable specifications for structural properties
- **Policy Packs**: Ready-to-use specifications for arithmetic, code safety, citations, medical, legal, PII protection, customer service, and financial domains
- **Reference Implementation**: [github.com/superficiallabs/cape](https://github.com/superficiallabs/cape)

More importantly: We're inviting the community to define capability. Contribute policy packs. Share rubrics. Extend the benchmark. The more contexts we encode, the more powerful this gets.

CAPE isn't a product. It's a protocol. And protocols get better when everyone builds on them.

## Quick Start: CAPE in 30 Minutes

1. **Clone the repo**: `git clone github.com/superficiallabs/cape`
2. **Pick a policy pack**: Start with Arithmetic or Citation (easiest to verify)
3. **Run the example**: `python examples/arithmetic.py`
4. **See the loop**: Watch Specify → Verify → Correct → Train in action
5. **Customize**: Edit `policies/arithmetic.json` to add your own requirements

Full tutorial: [docs.superficiallabs.com/cape/quickstart](https://docs.superficiallabs.com/cape/quickstart)

## The Path Forward

For too long, we've been trying to train generally capable models and hoping they'd figure out specific contexts. That's backwards.

Define what good means. Then train models to embody it.

General intelligence is a research problem. Specific capability is an engineering problem. CAPE provides the engineering.

**Capability isn't something you hope for. It's something you specify.**

## Get Started

- **Paper**: [Read the full technical report →](https://arxiv.org/abs/XXXX.XXXXX)
- **Code**: [github.com/superficiallabs/cape](https://github.com/superficiallabs/cape)
- **CapabilityBench**: [capabilitybench.com](https://capabilitybench.com)
- **Policy Packs**: Apache 2.0, ready to deploy
