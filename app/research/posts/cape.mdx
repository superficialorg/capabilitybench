---
title: 'Introducing CAPE: Capability Engineering for AI'
publishedAt: '2025-12-05'
summary: 'A new protocol for post-training that closes the gap between what models can do and what they will do.'
---

[Read the full paper ↗](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub ↗](https://github.com/superficiallabs/cape)

You deploy a coding assistant. It writes elegant functions, explains complex logic, suggests smart optimizations. Then it hardcodes an API key. Or uses a deprecated library your security team banned. Or formats dates as MM/DD/YYYY when your entire codebase uses DD/MM/YYYY.

The model isn't dumb—the code quality is excellent. But it doesn't know what "good code" means at your company.

This gap is why 67% of AI projects never make it to production. The model passes every benchmark but fails every deployment-specific requirement that actually matters.

## We've Been Solving the Wrong Problem

The field has spent billions trying to make models "universally good" at coding, legal research, customer service. But universal capability doesn't exist.

Good legal research at Cravath means something different than at a solo practice—different jurisdiction hierarchies, different citation standards, different escalation triggers. Good customer service at Zappos is different than at Goldman Sachs. Good code at your company follows your security policies, your style guide, your approved library list.

**Intelligence is general. Capability is contextual.**

We've been training for the wrong one.

Consider "good customer service." At the general level, it's genuinely subjective—annotators will disagree forever about what makes one response better than another.

But at your company, it decomposes into specific requirements:

- Acknowledge the customer's issue before offering solutions
- Offer escalation when sentiment indicates frustration
- Never promise specific timelines without checking availability
- Reference the customer's history when relevant
- Match formality to the customer's tone

These aren't vague guidelines. They're what good customer service means at your company. A model that satisfies them isn't just "not making mistakes"—it's being good at customer service in your context.

Our research confirms this: annotator agreement jumps from 42% ("Is this good customer service?") to 98% ("Does this satisfy these specific requirements?").

Context doesn't just reduce errors. It makes capability measurable.

## Why RLHF Hits a Ceiling

RLHF and DPO try to solve the general problem. They ask annotators: "Which response is better?"

But better for what? Better where? Better according to whom?

The results are predictable:

- Annotators disagree 30-50% of the time because they're bringing different contexts to the judgment
- Models learn averaged preferences that belong to no actual deployment context
- More compute just amplifies the disagreement—the ceiling is structural

You're not training toward a target. You're training toward an average of conflicting targets.

You end up with models that are generically pleasant but specifically useless—good at being inoffensive, bad at being right for your context.

## Introducing CAPE: Capability Achievement through Policy Execution

CAPE is a new protocol for post-training that inverts the problem: instead of trying to learn universal capability, you define what "good" means in your context, then train models to embody that definition.

The protocol has four steps that form a closed loop:

**1. Specify**: Define what "good" means in your context. Convert your requirements into executable policies (for structural properties like "cite cases from the correct jurisdiction") or explicit rubrics (for semantic properties like "reasoning quality").

**2. Verify**: Evaluate model outputs against your definition. For structural properties, verification is deterministic. For semantic properties, learned verifiers trained on your rubrics provide reliable assessment.

**3. Correct**: When outputs don't meet your standard, generate ones that do. Deterministic patching for clear fixes, constrained rewriting for judgment calls.

**4. Train**: Fine-tune on examples that embody your definition of good. The model internalizes what quality means in your context.

After training, the model doesn't just avoid mistakes—it thinks like your organization by default.

## CAPE Compounds

Here's what makes CAPE different from one-shot improvements: the loop feeds itself.

- Better verification → cleaner training signal
- Cleaner signal → better models
- Better models → improved verification
- Improved verification → more nuanced specifications

This isn't aspirational—it's empirical. Our learning curves show DPO plateauing at ~12% violation rate while CAPE continues improving throughout training. The gap widens over time.

And the ceiling keeps rising. Extraction fidelity improved from 72% to 96% in eighteen months as foundation models improved. Each percentage point translates to better capability transfer. CAPE gets better automatically as the infrastructure improves.

## The Unit Economics Just Changed Completely

RLHF requires annotators to judge every example. At $5-15 per preference comparison, a 10,000-example training run costs $50,000-150,000 in annotation alone.

CAPE requires writing specifications once, then applying them automatically.

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>RLHF/DPO</th>
      <th>CAPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Specification</td>
      <td>—</td>
      <td>$2,000-4,000</td>
    </tr>
    <tr>
      <td>Annotation</td>
      <td>$50,000-150,000</td>
      <td>—</td>
    </tr>
    <tr>
      <td>Verification infrastructure</td>
      <td>—</td>
      <td>$200-400</td>
    </tr>
    <tr>
      <td>Compute (training)</td>
      <td>$8,000-12,000</td>
      <td>$8,000-12,000</td>
    </tr>
    <tr>
      <td><strong>Total</strong></td>
      <td><strong>$80,000-200,000</strong></td>
      <td><strong>$10,000-16,000</strong></td>
    </tr>
    <tr>
      <td><strong>Timeline</strong></td>
      <td><strong>2-4 months</strong></td>
      <td><strong>1-2 weeks</strong></td>
    </tr>
  </tbody>
</table>

That 10-16x cost reduction isn't even the main point.

The main point is that your specifications become reusable assets. Your jurisdiction priorities. Your risk thresholds. Your escalation logic. Your documentation standards. Write once, apply to every training run, share across teams, compose into capability stacks.

With RLHF, you're renting generic helpfulness. With CAPE, you're encoding what makes you *you*.

Our arithmetic policy pack took 3 days to write and has generated training signal for 50,000+ examples. The legal pack encodes jurisdiction logic that would take months to capture through annotation. These aren't generic capabilities—they're your competitive advantage in executable form.

## Results: 82% Fewer Violations Than DPO

We trained models on 109,500 examples across six domains (arithmetic, code safety, legal citations, medical protocols, customer service, financial advisory).

CAPE reduced violation rates from 10-16% (DPO baseline) to 1.8-3.2%.

### Structural Verification (Violation Rates)

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Arithmetic</th>
      <th>Code Safety</th>
      <th>Citation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>31.4%</td>
      <td>38.2%</td>
      <td>35.7%</td>
    </tr>
    <tr>
      <td>DPO</td>
      <td>10.2%</td>
      <td>15.8%</td>
      <td>13.4%</td>
    </tr>
    <tr>
      <td>CAPE</td>
      <td>1.8%</td>
      <td>3.2%</td>
      <td>2.6%</td>
    </tr>
  </tbody>
</table>

### Semantic Verification (Rubric Scores, 0-1 scale)

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Argument Soundness</th>
      <th>Proof Validity</th>
      <th>Code Correctness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DPO</td>
      <td>0.62</td>
      <td>0.58</td>
      <td>0.64</td>
    </tr>
    <tr>
      <td>CAPE</td>
      <td>0.83</td>
      <td>0.81</td>
      <td>0.84</td>
    </tr>
  </tbody>
</table>

CAPE works across the spectrum—from deterministic structural properties to nuanced semantic judgments.

More importantly: CAPE keeps improving throughout training while DPO plateaus. The gap widens over time because verification quality compounds while preference disagreement remains fixed.

**Stability**: CAPE shows less than 0.3% variance across random seeds (vs. 1.6-2.1% for reward-based methods). When your training target is objective, your results are reproducible.

**Hybrid works**: CAPE → DPO achieves 2.9% violation rate and 63.7% preference win rate. Define correctness first, then optimize for fluency within those bounds.

## When to Use CAPE

CAPE excels when you can define what "good" means:

- **Domain-specific standards**: Your legal research process, your medical protocols, your coding conventions
- **Organizational requirements**: Your escalation logic, your documentation standards, your approval workflows
- **Structural correctness**: Arithmetic, citations, format compliance, API usage
- **Reasoning quality**: With explicit rubrics for what counts as sound

Preferences remain essential when "good" is genuinely subjective—aesthetic quality, creative style, humor.

Our analysis: 89% of deployment requirements become objective once context is fixed.

## Who Should Use CAPE

**You're an AI engineer at an enterprise**: You need models that follow your company's rules, not generic helpfulness. CAPE lets you encode your context without six-figure annotation budgets.

**You're a researcher**: You want to push the verification-capability frontier. CAPE gives you a protocol to test learned verifiers, rubric design, and compositional specifications.

**You're building a product**: Your users have domain-specific needs. CAPE lets you customize models for niche contexts without retraining from scratch.

## What We're Releasing

Everything under Apache 2.0:

- **CAPE Protocol**: The complete Specify → Verify → Correct → Train loop
- **CPL Specification Language**: Write executable policies for structural properties
- **Policy Packs**: Ready-to-use specifications for arithmetic, code safety, citations, medical protocols, legal research, PII protection, customer service, and financial advisory
- **Reference Implementation**: Full working system with examples

More importantly: we're inviting the community to define capability. Contribute policy packs. Share rubrics. Extend the benchmark. The more contexts we encode, the more powerful this gets.

CAPE isn't a product. It's a protocol. And protocols get better when everyone builds on them.

## Get Started in 30 Minutes

```bash
git clone https://github.com/superficiallabs/cape
cd cape
python examples/arithmetic.py
```

Watch the loop work: Specify → Verify → Correct → Train. Then customize `policies/arithmetic.json` to add your own requirements.

Full tutorial at [docs.superficiallabs.com/cape/quickstart](https://docs.superficiallabs.com/cape/quickstart)

---

The path forward is clear: Stop trying to train universally capable models and hoping they figure out your context. Define what "good" means in your world, then train models to embody it.

General intelligence is a research problem. Specific capability is an engineering problem.

**CAPE provides the engineering.**

## Resources

- [Read the paper ↗](https://arxiv.org/abs/XXXX.XXXXX)
- [GitHub repository ↗](https://github.com/superficiallabs/cape)
- [Documentation ↗](https://docs.superficiallabs.com/cape)
- [Policy pack library ↗](https://github.com/superficiallabs/cape/tree/main/policies)
