---
title: 'Introducing CAPE: Breaking Through the Post-Training Scaling Wall'
publishedAt: '2025-11-24'
summary: 'An open-source framework that replaces expensive preference annotation with LLM-assisted policy authoring, making post-training transparent, auditable, and accessible.'
---

At Superficial Labs, our goal is to accelerate human progress by making AI capable and reliable enough for the world's most important work. Over the past several years, the field has made enormous strides in pre-training and instruction-tuning, but one stage of the pipeline has become a persistent bottleneck: post-training. As models improve, the traditional methods used to refine their behaviour struggle to keep pace.

Today we are introducing CAPE (Capability Achievement via Policy Enforcement), an open-source, neurosymbolic post-training framework designed to address this problem directly. CAPE provides a more explicit, auditable, and scalable way to shape model behaviour by enforcing capability policies rather than relying on preference comparisons.

## The post-training scaling wall

Preference-based post-training methods such as RLHF and DPO assume that humans (or models) can reliably decide which of two model outputs is "better." This assumption held when models were relatively weak. As models became more consistent and generally correct, these comparative judgments became ambiguous, and annotator disagreement rose sharply.

This rising uncertainty has a practical consequence: the supervision signal becomes noisy enough that improvements in model capacity no longer translate into corresponding improvements in aligned behaviour. Several labs have observed the same pattern on structured, evaluable tasks such as arithmetic, code generation, tool use, and citation-grounded QA.

This plateau is now a central limitation for models intended for scientific, medical, financial, or other safety-critical domains where correctness and auditability matter.

## CAPE: policies instead of preferences

CAPE reframes the question from "Which output is better?" to "Does the output satisfy the required conditions?"

To do this, CAPE introduces a closed-loop neurosymbolic post-training process:

1. **Extraction**

Model outputs are converted into a structured, versioned representation called a PredicateGraph, capturing entities, claims, reasoning steps, tool-use structure, citations, safety markers, and discourse acts.

2. **Policy evaluation**

Domain requirements are expressed as executable capability policies — deterministic checks over the PredicateGraph describing what must hold for the output to be considered correct, safe, or structurally compliant.

3. **Correction**

When a policy is violated, CAPE applies minimal deterministic fixes or constrained rewrites.

4. **Learning**

The model trains directly on corrected outputs, gradually internalising the policies and reducing violations over time.

Because the supervision signal is defined explicitly by policies rather than implicitly by human comparisons, its quality scales with extractor fidelity — an engineering variable that can improve with better models, synthetic data, ensembles, and domain-specific refinement.

## Empirical results

We applied CAPE across three domains: arithmetic with tool calls, safe Python code generation, and citation-grounded question answering. In all cases, preference-based methods reduce violation rates early in training but plateau between 10–20%.

CAPE continues improving beyond these plateaus, converging instead to extractor-limited floors (typically 2–8%). Importantly, the remaining violations correspond to extraction errors rather than model capacity limits.

This pattern suggests that CAPE restores predictable scaling to post-training, even as models become more capable.

## Complementarity with preference learning

While CAPE provides a deterministic and auditable foundation for correctness, safety, governance, and structural requirements, preference learning remains valuable for subjective qualities such as tone, phrasing, and stylistic preferences.

We therefore recommend a two-phase approach:

1. CAPE for structural, correctness, and safety constraints

2. Preference learning for subjective quality within those constraints

This layering preserves the strengths of both methods and yields models better suited for high-stakes applications.

## What we're releasing

Today we are releasing CAPE-Core, an open-source implementation that includes:

* the PredicateGraph schema (versioned and extensible)

* reference extractors for arithmetic, code, and QA

* a deterministic policy engine with semantic routing

* LLM-assisted policy authoring tools

* a correction engine supporting literal fixes, templates, and constrained rewrites

* starter policy packs for common domains

* an evaluation harness for measuring violation rates and extractor performance

* documentation for domain experts, not only ML practitioners

CAPE-Core is available under a permissive license to support research, enterprise integration, and collaborative policy development.

## Looking ahead

For AI to meaningfully advance the world's most important work, capability and reliability must progress together. CAPE offers a way to make post-training more transparent, maintainable, and fundamentally scalable — traits that become increasingly necessary as models are deployed in regulated, safety-critical, and professional settings.

We invite researchers, practitioners, and domain experts to explore CAPE, contribute new policies and extractors, and help build a foundation for more reliable AI systems.

Read the paper → /mnt/data/CAPE.pdf

[Try CAPE-Core →](https://github.com/superficiallabs/cape-core)

[Contact us →](mailto:research@superficiallabs.com)
