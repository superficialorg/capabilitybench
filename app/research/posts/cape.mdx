---
title: 'Introducing CAPE: Breaking Through the Post-Training Scaling Wall'
publishedAt: '2025-11-24'
summary: 'An open-source framework that replaces expensive preference annotation with LLM-assisted policy authoring, making post-training transparent, auditable, and accessible.'
---

Our goal is to accelerate human progress by making AI capable and reliable enough for the world's most important work.

Today we present CAPE (Capability Achievement via Policy Enforcement): an open-source framework that replaces traditional preference-based post-training with explicit, executable policy enforcement. CAPE enables transparent, auditable, and continually improving model behaviour.

---

## The Scaling Wall in Post-Training

Over the past decade we've witnessed tremendous gains in AI through pre-training and instruction-tuning. However, the final phase — post-training alignment — is now hitting a persistent barrier.

Conventional pipelines ask humans to compare model outputs, infer which is "better," train a reward model, and then fine-tune. But when model quality is already high, these judgments become inconsistent and noisy. Disagreement rates creep toward 50 %, and the effective training signal collapses. The result: models plateau, even though raw capacity continues to increase.

This scaling wall is especially acute in structured, evaluable domains: arithmetic, code generation, tool use, citation-based QA. At Superficial we believe that unless this wall is broken, AI cannot reliably serve domains where correctness, safety, and auditability matter most.

## A Shift in Perspective: From Preferences to Policies

CAPE pivots the question from "Which output is better?" to "Does this output satisfy our defined requirements?"

Here's how:

* We convert model outputs into a structured representation called a PredicateGraph — capturing entities, claims, tool calls, reasoning steps, citations, discourse acts, and style markers.

* We express requirements as deterministic, executable policies — code-checks over the graph (e.g., "tool call arguments must match arithmetic operations", "medical advice for acute symptoms must include escalation", "factual claims must be cited").

* We detect violations, apply corrections (literal fixes, templates, constrained rewrites), then train the model on the corrected outputs.

* Over time the model learns to produce outputs that satisfy the policies without intervention.

By making the supervision signal explicit and engineered, rather than ambiguous and preference-based, CAPE restores signal strength and alignment in post-training.

## Why This Matters

CAPE delivers four core benefits that align with our mission:

* **Restored training signal**: The limiting factor becomes extraction fidelity (an engineering problem) rather than human disagreement (an irreducible noise source).

* **Lower operational cost**: Domain experts can author natural-language requirements, LLMs draft policies, validations occur on small sets — instead of large annotation campaigns.

* **Reuse of investment**: Existing preference datasets become sources of structural invariants — distilled into reusable policy libraries rather than one-off reward models.

* **Comprehensive enforcement**: CAPE covers objective correctness (Tier 1), safety and governance (Tier 2), and structural style/voice (Tier 3) within one unified system.

In regulated, high-stakes environments (healthcare, finance, legal, enterprise deployments) reliability and auditability matter as much as raw capability. CAPE aligns with that need.

## Empirical Evidence

In our experiments across three domains — arithmetic with tool calls, safe Python code generation, and citation-constrained question answering — we found:

* Traditional methods (DPO, RLHF) drive violation rates down initially but plateau near 10–20%.

* With CAPE, violation rates continue decreasing until they reach 2–8 % — almost entirely bounded by extractor fidelity, not model capacity.

* Residual error correlates strongly with extraction mistakes rather than capability limits.

In other words: CAPE keeps improving where preference-based approaches stall.

## What We're Releasing

We are open-sourcing CAPE-Core, which includes:

* The PredicateGraph schema (versioned, extensible)

* Reference extractors for arithmetic, code and QA tasks

* A policy engine with semantic routing and deterministic evaluation

* LLM-assisted policy authoring tools (templates, validation harness)

* A correction engine supporting literal fixes, templates, constrained rewrites

* Starter policy packs (arithmetic, code safety, citation integrity)

* An evaluation harness for benchmarking violation rates and extractor performance

* Documentation and tutorials designed for domain experts (without ML team reliance)

All under a permissive license. We aim to build a shared ecosystem of policy-packs, extractors, and model workflows.

## What's Next

CAPE is not a replacement for preference learning — it is a foundation for making preference learning viable again.

We recommend a two-phase pipeline:

1. Phase 1: Apply CAPE to enforce correctness, safety, governance, structure.

2. Phase 2: Use preference learning inside that constrained space to optimise subjective quality (tone, style, empathy, brand voice).

This hybrid approach enables models that are both sound and excellent. That's exactly what it takes if AI is to be trusted for the world's most important work.

## Get Involved

* Read the paper → file://mnt/data/CAPE%20(Superficial%20Labs).pdf

* [Try CAPE-Core →](https://github.com/superficiallabs/cape-core)

We're particularly interested in collaborations involving:

* New domain-specific extractors (medical, legal, scientific)

* Policy pack development for new sectors and tasks

* Improvements in extraction methods and benchmarks

* Case studies from real-world deployments

CAPE is our step toward reliable, auditable, high-capability AI. We invite the research community, enterprises, and domain experts to build the future with us.
