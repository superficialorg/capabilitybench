---
title: 'Introducing CAPE: Capability Engineering for AI'
publishedAt: '2025-12-05'
summary: 'A systematic practice for making models capable of what users and organizations actually need, reducing violation rates by 81–99% relative to DPO and cutting costs by 20–200×.'
---

[Read the full paper →](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub →](https://github.com/superficiallabs/cape)

A model that wins mathematical olympiads cannot reliably follow a hospital formulary. A model that writes publishable code cannot guarantee it will cite only the documents you provide. A model that passes the bar exam cannot enforce your firm's jurisdiction constraints.

This is the Deployment Gap: the chasm between what models can do in principle and what they can be trusted to do in practice. We have spent the last three years scaling intelligence, but we haven't scaled capability.

**Intelligence** is the ability to solve complex, open-ended problems. **Capability** is the reliable satisfaction of specific requirements. A model can be extraordinarily intelligent and yet lack the capabilities that deployment demands. Intelligence is necessary but insufficient. Capability must be engineered.

Today we release CAPE (Capability Achievement via Policy Execution) and introduce **Capability Engineering**: a systematic practice for making models capable of what users and organizations actually need.

## Why Preference-Based Training Has Hit a Wall

Post-training methods like RLHF optimize for intelligence. They ask annotators: "Which response is better?" But annotators disagree 30–50% of the time on subtle judgments<sup>1</sup>. This isn't fixable with better guidelines or more training—it reflects genuine variance in human judgment when requirements are underspecified.

Beyond disagreement, reward-based methods suffer from structural algorithmic biases:

**PPO exhibits length bias**: When a model receives negative reward, longer responses dilute the per-token penalty. The model learns that verbosity reduces punishment, even when extra tokens don't help correctness<sup>2</sup>.

**GRPO suffers from difficulty-level bias**: Normalizing by reward standard deviation causes easy or hard questions to be overweighted. These require algorithmic patches like Dr. GRPO that add complexity without guaranteeing fixes<sup>3</sup>.

The consequences compound:

- **More compute amplifies noise and bias.** Training harder doesn't converge to correctness; it overfits to inconsistency.

- **Costs are prohibitive.** Preference annotation runs $50K–200K per capability area with timelines stretching to months.

- **The ceiling is fixed.** No amount of investment overcomes the preference plateau.

CAPE sidesteps these pathologies entirely. Verification produces binary pass/fail verdicts per policy. There is no advantage normalization, no length-based reward shaping, no difficulty weighting. The training signal is direct: "this output satisfies the specification."

## Contextual Objectivity

A common objection to specification-based approaches is: "You can't write specifications for the things that matter. Real requirements are too subjective."

This is wrong. It confuses *abstract* with *subjective*.

"Good medical advice" is abstract. It means different things to different people. But "recommend only drugs from this formulary, flag contraindications from the patient record, verify dosages against weight-based limits" is objective. There is a fact of the matter. It can be verified.

We call this **Contextual Objectivity**: the recognition that properties appearing subjective become objective once context is fixed. Our inter-annotator studies confirm this empirically:

<table>
  <thead>
    <tr>
      <th>Condition</th>
      <th>Agreement (κ)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Abstract ("Is this good medical advice?")</td>
      <td>0.42 (63%)</td>
    </tr>
    <tr>
      <td>Contextualized ("Formulary only, flag contraindications")</td>
      <td>0.73 (85%)</td>
    </tr>
    <tr>
      <td>Explicit Policy (executable specification)</td>
      <td>0.98 (99%)</td>
    </tr>
  </tbody>
</table>

The disagreement wasn't about values. It was about underspecified requirements. Fix the context, and agreement follows<sup>4</sup>.

## The Verification-Fidelity Scaling Law

The binding constraint on post-training is not preference agreement but verification fidelity.

### The Preference Ceiling is Structural

- **Annotator disagreement**: 30–50% fixed.
- **No path forward**: More compute amplifies noise rather than reducing it.

### The Verification Ceiling Falls With Scale

CAPE's ceiling is verification fidelity, which improves with scale:

- **Structural Verification**: Extraction fidelity has improved from 72% (GPT-3.5) to 96% (Claude Opus 4.5). Correlation with violation rate is r=0.94<sup>5</sup>.

- **Semantic Verification**: Verifier correlation with expert judgment improved from r=0.71 (1B models) to r=0.87 (8B models)<sup>6</sup>.

### The Critical Asymmetry:

<table>
  <thead>
    <tr>
      <th>Error Source</th>
      <th>Scales?</th>
      <th>Evidence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Preference disagreement</td>
      <td>No</td>
      <td>30–50% fixed</td>
    </tr>
    <tr>
      <td>Algorithmic bias (PPO/GRPO)</td>
      <td>No</td>
      <td>Requires patches</td>
    </tr>
    <tr>
      <td>Extraction error</td>
      <td>Yes</td>
      <td>14.8% → 5.5%</td>
    </tr>
    <tr>
      <td>Verifier error</td>
      <td>Yes</td>
      <td>r: 0.71 → 0.87</td>
    </tr>
  </tbody>
</table>

As models improve, CAPE becomes not just competitive with preference learning but the only method whose upper bound moves with model scale.

## How CAPE Works

CAPE implements a closed loop: **Specify → Verify → Correct → Train.**

### 1. Specify

Requirements become executable. We use CPL (CAPE Policy Language) for structural properties and learned verifiers for semantic ones.

Example CPL policy:

```json
{
  "id": "policy.tool.calc_matches",
  "tier": "T1",
  "scope": {"kind": "tool_call", "filter": {"name": "calc"}},
  "where": [{"expr": "count(operations) > 0"}],
  "assert": [{
    "expr": "tool_call.arguments.value == last(operations).output"
  }],
  "on_violation": {
    "action": "CORRECT",
    "correction_hint": "Update to exact value"
  }
}
```

### 2. Verify

Model outputs are parsed into a PredicateGraph and evaluated. Symbolic policies execute deterministically: same output, same verdict, every time.

### 3. Correct

Violations trigger targeted fixes.

- **Deterministic patches**: 99.7% success
- **Template insertion**: 97.3% success
- **Constrained rewrites**: 94.6% success

### 4. Train

Corrected outputs become supervised training signal. No preference labels required.

## What This Unlocks

### Post-training for everyone

Preference annotation costs $50K–200K per capability area. CAPE costs ~$500. Timelines drop from months to weeks.

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>RLHF/DPO</th>
      <th>CAPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Specification</td>
      <td>1–2 weeks</td>
      <td>1–3 days</td>
    </tr>
    <tr>
      <td>Data Cost</td>
      <td>$20K–50K</td>
      <td>~$200</td>
    </tr>
    <tr>
      <td>Training</td>
      <td>1–2 weeks</td>
      <td>1–3 days</td>
    </tr>
    <tr>
      <td><strong>Total Cost</strong></td>
      <td><strong>$50K–200K</strong></td>
      <td><strong>~$500</strong></td>
    </tr>
    <tr>
      <td><strong>Timeline</strong></td>
      <td><strong>3–6 months</strong></td>
      <td><strong>1–2 weeks</strong></td>
    </tr>
  </tbody>
</table>

### Engineering, not alchemy

Traditional software has specifications, test suites, and traceable fixes. CAPE brings this discipline to AI. When a model fails in production, you can trace the failure to a specific policy.

## Results

Across 109,500 examples in six domains, CAPE reduces violation rates by 81–99% relative to DPO.

### Structural Verification

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Arithmetic</th>
      <th>Code Safety</th>
      <th>Citation</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>31.4%</td>
      <td>38.2%</td>
      <td>35.7%</td>
      <td>35.1%</td>
    </tr>
    <tr>
      <td>DPO</td>
      <td>10.2%</td>
      <td>15.8%</td>
      <td>13.4%</td>
      <td>13.1%</td>
    </tr>
    <tr>
      <td>CAPE (frontier)</td>
      <td>1.8%</td>
      <td>3.2%</td>
      <td>2.6%</td>
      <td>2.5%</td>
    </tr>
  </tbody>
</table>

### Semantic Verification

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Argument</th>
      <th>Proof Validity</th>
      <th>Code Correctness</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>0.54</td>
      <td>0.51</td>
      <td>0.57</td>
      <td>0.54</td>
    </tr>
    <tr>
      <td>DPO</td>
      <td>0.62</td>
      <td>0.58</td>
      <td>0.64</td>
      <td>0.61</td>
    </tr>
    <tr>
      <td>CAPE (R3)</td>
      <td>0.83</td>
      <td>0.81</td>
      <td>0.84</td>
      <td>0.83</td>
    </tr>
  </tbody>
</table>

With frontier extractors, residual violations fall to 2.5%. Critically, CAPE exhibits high stability: σ < 0.3% across random seeds, compared to 1.6–2.1% for reward-based methods.

## CapabilityBench: A Call to the Community

Current benchmarks measure intelligence with opaque aggregate scores. A model scoring 78% tells you nothing about whether it can satisfy your specific requirements.

CapabilityBench is a public capability registry at capabilitybench.com. We are releasing the policy pack specification and initial packs under Apache 2.0.

Rather than a single score, CapabilityBench produces an **Adherence Profile**:

- **Core Adherence**: % of test cases satisfying all core policies.
- **Violation Distribution**: Which specific policies fail and how often.

We are releasing eight initial policy packs (Medical, Legal, PII-Protect, etc.), but the vision is larger: a shared, growing repository of capability specifications across every domain that matters.

## Conclusion

We don't need more intelligent models to solve today's deployment failures. We need more capable ones.

**Intelligence training asks**: "Can the model solve this?"

**Capability engineering asks**: "Does the model satisfy this specification?"

A model can score 79.8% on AIME and still recommend off-formulary medications. Intelligence is necessary but insufficient. Capability must be engineered.

We're releasing CAPE openly because we believe the answer to capable AI is not proprietary methods, but rigorous frameworks developed in public. Executable specifications are auditable by design. That auditability should be a shared resource.

**Capability must be engineered. Now it can be.**

[Read the full paper →](https://arxiv.org/abs/XXXX.XXXXX) | [GitHub →](https://github.com/superficiallabs/cape) | [CapabilityBench →](https://capabilitybench.com)
