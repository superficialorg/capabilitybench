---
title: 'Introducing CAPE: Breaking Through the Post-Training Scaling Wall'
publishedAt: '2025-11-24'
summary: 'An open-source framework that replaces expensive preference annotation with LLM-assisted policy authoring, making post-training transparent, auditable, and accessible.'
---

Our goal is to accelerate human progress by making AI capable and reliable enough for the world's most important work.

Over the last several years, pre-training and instruction tuning have continued to improve at remarkable speed. But one stage of the pipeline increasingly lags behind: post-training, the process responsible for shaping model behaviour into something safe, correct, compliant, and usable.

Today we're releasing CAPE (Capability Achievement via Policy Enforcement), a transparent and scalable post-training framework that replaces preference-based supervision with explicit, executable capability policies. CAPE restores a deterministic supervision signal to post-training, allowing capability and reliability to scale together even as models become more powerful.

CAPE-Core — the open-source reference implementation — accompanies this release and is available for researchers and practitioners to test, extend, and adopt.

## The Post-Training Scaling Wall

Modern post-training relies heavily on reinforcement learning from human feedback (RLHF) and Direct Preference Optimization (DPO): given two model outputs, a human or model judge selects the preferred one, and the model learns to imitate that preference.

As models improve, this process breaks down.

When most outputs are "good" in an absolute sense, annotators disagree more frequently about which output is better. Under standard preference models, the useful signal scales like 1 – 2δ, where δ is the disagreement rate. As δ approaches 50%, the gradient vanishes. Additional compute does not translate into proportional improvement.

This is not an annotation problem — it is a structural one. Preference-based supervision is non-deterministic, and non-deterministic gradients do not compound as models improve.

Pre-training scales so well because its supervision signal — next-token prediction — is deterministic. Every example produces a well-defined gradient. Post-training today does not enjoy that property.

As a result, RLHF and DPO have reached a post-training scaling wall, especially on structured, evaluable tasks such as arithmetic with tool calls, code safety, and citation-grounded QA. Models could improve, but the supervision signal can't scale with them.

## CAPE: Policies Instead of Preferences

CAPE reframes the core task of post-training:

* Traditional methods ask: Which of these two outputs is better?

* CAPE asks: Does this output satisfy our requirements?

CAPE introduces a closed-loop, neurosymbolic post-training pipeline:

1. **Extraction**

An LLM maps the model's output into a structured, versioned PredicateGraph.

This graph captures entities, claims, reasoning steps, arithmetic and logical operations, tool calls, discourse acts, safety markers, citations, and stylistic structure.

2. **Policy Evaluation**

Domain requirements are expressed as executable capability policies.

Policies operate over the PredicateGraph, evaluating whether correctness, safety, compliance, and structural constraints are satisfied.

3. **Deterministic Correction**

If a policy is violated, CAPE applies minimal deterministic fixes or constrained rewrites to produce a corrected output.

4. **Learning**

The corrected output becomes a clean, unambiguous training target.

This loop — extraction → evaluation → correction → learning — restores a deterministic supervision regime to post-training. As long as a capability can be expressed as executable predicates over a graph, CAPE can train a model to achieve it.

## Why CAPE Is Feasible Today

Two years ago, predicate extraction was too unreliable for this approach to work. Today, advances in frontier LLMs have changed the landscape:

* High-fidelity structured extraction across text, code, math, and citations

* Stable, schema-conforming generation

* Accurate mapping of discourse and safety markers

* Stronger generalization across domains

* LLMs capable of drafting executable policies with minimal human editing

These capabilities have crossed the threshold required for predicate-graph-based enforcement to outperform preference-based supervision on many structured tasks.

## Empirical Results

We apply CAPE to three structured domains:

* arithmetic with tool calls

* safe Python code generation

* citation-constrained QA

Across all domains, preference-based methods (DPO, outcome-based RL) reduce violations early but plateau between 10–20%. CAPE continues improving beyond these plateaus.

Violation rates under CAPE follow an approximate power-law decay until reaching extractor-limited floors of 2–8%, depending on extractor quality. The remaining violations correspond closely to extraction errors, not model limitations.

This matches the central theory: when supervision is deterministic, improvement compounds until the extraction floor is reached. When supervision is non-deterministic, gradients collapse far earlier.

## How CAPE Restores Post-Training Scaling

CAPE's core advantage is that the supervision signal is bounded by extraction fidelity (ε) rather than human disagreement (δ).

When ε < δ — which is now true for many structured tasks — CAPE provides a strictly stronger, more scalable signal than RLHF or DPO.

This is the main conceptual contribution:

CAPE turns post-training into a deterministic, machine-speed, high-bandwidth training loop.

## CAPE + Preference Learning

CAPE is not a replacement for preference learning.

Instead, CAPE provides the necessary structural, correctness, and safety guarantees, while preference learning provides subjective refinement.

We recommend a two-phase hybrid:

1. CAPE for correctness, safety, governance, and structural scaffolding

2. Preference learning for style, tone, empathy, and domain-specific nuance

CAPE establishes the floor. Preferences lift the ceiling.

## What We're Releasing

Today we are releasing CAPE-Core, an open-source reference implementation that includes:

* The PredicateGraph schema (versioned and extensible)

* Reference extractors for arithmetic, code, and QA

* A deterministic policy engine with semantic routing

* LLM-assisted policy authoring tools and templates

* A correction engine supporting literal edits, templates, and constrained rewrites

* Starter policy packs

* An evaluation harness for measuring violation rates and extractor performance

* Documentation designed for domain experts as well as ML practitioners

CAPE-Core aims to provide a foundation for a growing policy ecosystem:

shared policy packs, improved extractors, richer ontologies, and best practices for policy authoring.

## Outlook

For AI to contribute meaningfully to the world's most important work, capability and reliability must advance together. CAPE operationalises this vision by giving post-training a deterministic, transparent, and maintainable foundation.

Even if CAPE ultimately applies "only" to structured and evaluable capabilities, that subset includes precisely the tasks where correctness, safety, and auditability matter most. For those domains, CAPE provides a path to continue improving model behaviour long after preference-based methods reach their limits.

We invite researchers, enterprises, and domain experts to explore CAPE and help expand the policy ecosystem.

## Get Involved

Read the paper → /mnt/data/CAPE.pdf

[Try CAPE-Core →](https://github.com/superficiallabs/cape-core)

[Contact us →](mailto:research@superficiallabs.com)
